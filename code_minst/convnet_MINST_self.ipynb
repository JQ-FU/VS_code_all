{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torchvision\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([60000, 28, 28])\n",
      "Labels shape: torch.Size([60000])\n",
      "Data : tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 假设数据集的路径\n",
    "data_path = 'D:\\code\\classifier_data'\n",
    "\n",
    "# 定义数据转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为 Tensor\n",
    "    transforms.Normalize((0,), (255,))  # 归一化\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "mnist_dataset = datasets.MNIST(root=data_path, train=True, transform=transform, download=False)\n",
    "\n",
    "# 将数据和标签分离\n",
    "data = mnist_dataset.data\n",
    "labels = mnist_dataset.targets\n",
    "\n",
    "# 将数据和标签转换为 Tensor\n",
    "data_all = data.unsqueeze(1)  # 添加通道维度 (N, C, H, W)，其中 C = 1\n",
    "label_all = labels.long()  # 确保标签为 long 类型\n",
    "\n",
    "# 打印数据和标签的形状\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Data :\", data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,h1=96):\n",
    "        # We optimize dropout rate in a convolutional neural network.\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.drop1=nn.Dropout2d(p=0.5)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, h1)\n",
    "        self.drop2=nn.Dropout2d(p=0.1)\n",
    "        self.fc2 = nn.Linear(h1, 10)\n",
    "    def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x),kernel_size = 2))\n",
    "            x = F.relu(F.max_pool2d(self.conv2(x),kernel_size = 2))\n",
    "            x = self.drop1(x)\n",
    "            x = x.view(x.size(0),-1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.drop2(x)\n",
    "            x = self.fc2(x)\n",
    "            return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs=10\n",
    "batch_size=256\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fn,optimizer):\n",
    "  train_loss,train_correct=0.0,0\n",
    "  model.train()\n",
    "  for images, labels in dataloader:\n",
    "    images,labels = images.to(device),labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(images)\n",
    "    loss = loss_fn(output,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item() * images.size(0)\n",
    "    scores, predictions = torch.max(output.data, 1)\n",
    "    train_correct += (predictions == labels).sum().item()\n",
    "  return train_loss,train_correct\n",
    "def valid_epoch(model,device,dataloader,loss_fn):\n",
    "        valid_loss, val_correct = 0.0, 0\n",
    "        model.eval()\n",
    "        for images, labels in dataloader:\n",
    "            images,labels = images.to(device),labels.to(device)\n",
    "            output = model(images)\n",
    "            loss=loss_fn(output,labels)\n",
    "            valid_loss+=loss.item()*images.size(0)\n",
    "            scores, predictions = torch.max(output.data,1)\n",
    "            val_correct+=(predictions == labels).sum().item()\n",
    "        return valid_loss,val_correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FJQ\\AppData\\Local\\Temp\\ipykernel_32880\\889207555.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(epoch_train, dtype=torch.float32), torch.tensor(label_train, dtype=torch.long))\n",
      "C:\\Users\\FJQ\\AppData\\Local\\Temp\\ipykernel_32880\\889207555.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(epoch_val, dtype=torch.float32), torch.tensor(label_val, dtype=torch.long))\n",
      "d:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:1/10 AVG Training Loss:0.829 AVG Test Loss:0.131 AVG Training Acc 78.58 % AVG Test Acc 95.98 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:2/10 AVG Training Loss:0.180 AVG Test Loss:0.075 AVG Training Acc 94.43 % AVG Test Acc 97.80 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:3/10 AVG Training Loss:0.137 AVG Test Loss:0.064 AVG Training Acc 95.84 % AVG Test Acc 97.98 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:4/10 AVG Training Loss:0.112 AVG Test Loss:0.054 AVG Training Acc 96.60 % AVG Test Acc 98.24 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:5/10 AVG Training Loss:0.096 AVG Test Loss:0.053 AVG Training Acc 97.11 % AVG Test Acc 98.35 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:6/10 AVG Training Loss:0.089 AVG Test Loss:0.053 AVG Training Acc 97.29 % AVG Test Acc 98.43 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:7/10 AVG Training Loss:0.079 AVG Test Loss:0.046 AVG Training Acc 97.58 % AVG Test Acc 98.76 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:8/10 AVG Training Loss:0.079 AVG Test Loss:0.060 AVG Training Acc 97.61 % AVG Test Acc 98.22 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:9/10 AVG Training Loss:0.071 AVG Test Loss:0.048 AVG Training Acc 97.77 % AVG Test Acc 98.59 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:10/10 AVG Training Loss:0.070 AVG Test Loss:0.050 AVG Training Acc 97.82 % AVG Test Acc 98.61 %\n",
      "Fold 2\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:1/10 AVG Training Loss:0.638 AVG Test Loss:0.082 AVG Training Acc 85.31 % AVG Test Acc 97.39 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:2/10 AVG Training Loss:0.150 AVG Test Loss:0.061 AVG Training Acc 95.57 % AVG Test Acc 98.19 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:3/10 AVG Training Loss:0.117 AVG Test Loss:0.051 AVG Training Acc 96.48 % AVG Test Acc 98.52 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:4/10 AVG Training Loss:0.100 AVG Test Loss:0.045 AVG Training Acc 96.97 % AVG Test Acc 98.57 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:5/10 AVG Training Loss:0.091 AVG Test Loss:0.048 AVG Training Acc 97.19 % AVG Test Acc 98.50 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:6/10 AVG Training Loss:0.082 AVG Test Loss:0.043 AVG Training Acc 97.58 % AVG Test Acc 98.65 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:7/10 AVG Training Loss:0.076 AVG Test Loss:0.037 AVG Training Acc 97.67 % AVG Test Acc 98.89 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:8/10 AVG Training Loss:0.073 AVG Test Loss:0.041 AVG Training Acc 97.72 % AVG Test Acc 98.69 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:9/10 AVG Training Loss:0.070 AVG Test Loss:0.038 AVG Training Acc 97.85 % AVG Test Acc 98.81 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:10/10 AVG Training Loss:0.064 AVG Test Loss:0.044 AVG Training Acc 97.97 % AVG Test Acc 98.69 %\n",
      "Fold 3\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:1/10 AVG Training Loss:0.602 AVG Test Loss:0.089 AVG Training Acc 84.81 % AVG Test Acc 97.19 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:2/10 AVG Training Loss:0.168 AVG Test Loss:0.071 AVG Training Acc 94.93 % AVG Test Acc 97.70 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:3/10 AVG Training Loss:0.131 AVG Test Loss:0.058 AVG Training Acc 96.08 % AVG Test Acc 98.31 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:4/10 AVG Training Loss:0.114 AVG Test Loss:0.049 AVG Training Acc 96.52 % AVG Test Acc 98.37 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:5/10 AVG Training Loss:0.097 AVG Test Loss:0.046 AVG Training Acc 97.09 % AVG Test Acc 98.50 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:6/10 AVG Training Loss:0.087 AVG Test Loss:0.040 AVG Training Acc 97.30 % AVG Test Acc 98.48 %\n",
      "train_loss type: <class 'float'>, train_correct type: <class 'int'>\n",
      "Epoch:7/10 AVG Training Loss:0.084 AVG Test Loss:0.034 AVG Training Acc 97.45 % AVG Test Acc 98.69 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m:[],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 38\u001b[0m     train_loss, train_correct\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_correct type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train_correct)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     test_loss, test_correct\u001b[38;5;241m=\u001b[39mvalid_epoch(model,device,test_loader,criterion)\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, device, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m train_loss,train_correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      5\u001b[0m   images,labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device),labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\anaconda\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(data_all)))):\n",
    "  #print(train_idx.shape,val_idx.shape)\n",
    "  \n",
    "  print('Fold {}'.format(fold + 1))\n",
    "##############################\n",
    "  n = len(train_idx)\n",
    "  A = np.linspace(0, n - 1, n, dtype=int)\n",
    "  random.shuffle(A)\n",
    "  epoch_train = data_all[train_idx[A[:int(0.9 * n)]]]\n",
    "  epoch_val = data_all[train_idx[A[int(0.9 * n):]]]\n",
    "  epoch_test = data_all[val_idx]\n",
    "  label_train = label_all[train_idx[A[:int(0.9 * n)]]]\n",
    "  label_val = label_all[train_idx[A[int(0.9 * n):]]]\n",
    "  label_test = label_all[val_idx]\n",
    "\n",
    "    # 转换为Tensor并创建DataLoader\n",
    "  train_dataset = TensorDataset(torch.tensor(epoch_train, dtype=torch.float32), torch.tensor(label_train, dtype=torch.long))\n",
    "  val_dataset = TensorDataset(torch.tensor(epoch_val, dtype=torch.float32), torch.tensor(label_val, dtype=torch.long))\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#############################\n",
    "  #train_sampler = SubsetRandomSampler(train_idx)\n",
    "  #test_sampler = SubsetRandomSampler(val_idx)\n",
    "  \n",
    "  #train_loader = DataLoader(data_all, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "  #test_loader = DataLoader(data_all, batch_size=batch_size, sampler=test_sampler)\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = ConvNet()\n",
    "  model.to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "  history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    " \n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
    "      print(f'train_loss type: {type(train_loss)}, train_correct type: {type(train_correct)}')\n",
    "      test_loss, test_correct=valid_epoch(model,device,test_loader,criterion)\n",
    "      train_loss = train_loss / len(train_loader.sampler)\n",
    "      train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "      test_loss = test_loss / len(test_loader.sampler)\n",
    "      test_acc = test_correct / len(test_loader.sampler) *100\n",
    "      print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                                              num_epochs,\n",
    "                                                                                                                              train_loss,\n",
    "                                                                                                                              test_loss,\n",
    "                                                                                                                              train_acc,\n",
    "                                                                                                                              test_acc))\n",
    "      history['train_loss'].append(train_loss)\n",
    "      history['test_loss'].append(test_loss)\n",
    "      history['train_acc'].append(train_acc)\n",
    "      history['test_acc'].append(test_acc)\n",
    "      foldperf['fold{}'.format(fold+1)] = history\n",
    "      torch.save(model,'k_cross_CNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.180 \t Average Test Loss: 0.058 \t Average Training Acc: 94.95 \t Average Test Acc:98.08\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.164 \t Average Test Loss: 0.060 \t Average Training Acc: 95.48 \t Average Test Acc:98.13\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.159 \t Average Test Loss: 0.061 \t Average Training Acc: 95.59 \t Average Test Acc:98.12\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.170 \t Average Test Loss: 0.063 \t Average Training Acc: 95.29 \t Average Test Acc:98.09\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.164 \t Average Test Loss: 0.063 \t Average Training Acc: 95.41 \t Average Test Acc:98.13\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.173 \t Average Test Loss: 0.063 \t Average Training Acc: 95.15 \t Average Test Acc:98.11\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.176 \t Average Test Loss: 0.064 \t Average Training Acc: 95.07 \t Average Test Acc:98.09\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.175 \t Average Test Loss: 0.063 \t Average Training Acc: 95.07 \t Average Test Acc:98.10\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.177 \t Average Test Loss: 0.064 \t Average Training Acc: 95.03 \t Average Test Acc:98.10\n",
      "Performance of 10 fold cross validation\n",
      "Average Training Loss: 0.178 \t Average Test Loss: 0.064 \t Average Training Acc: 94.99 \t Average Test Acc:98.10\n"
     ]
    }
   ],
   "source": [
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=10\n",
    "for f in range(1,k+1):\n",
    "  tl_f.append(np.mean(foldperf['fold{}'.format(f)]['train_loss']))\n",
    "  testl_f.append(np.mean(foldperf['fold{}'.format(f)]['test_loss']))\n",
    "  ta_f.append(np.mean(foldperf['fold{}'.format(f)]['train_acc']))\n",
    "  testa_f.append(np.mean(foldperf['fold{}'.format(f)]['test_acc']))\n",
    "  print('Performance of {} fold cross validation'.format(k))\n",
    "  print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc:{:.2f}\".format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
